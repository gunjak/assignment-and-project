{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When higher the number of features in a dataset, the harder it gets to visualize the training set and then work on it. Sometimes\n",
    ", most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to \n",
    "the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions. A\n",
    "higher number of dimensions theoretically allow more information to be stored, but practically it rarely helps due to the higher\n",
    "possibility of noise and redundancy in the real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at\n",
    "least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too\n",
    "much information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Depends on the dataset!If all the variance is in a few dimensions this can be very low If the instances of original dataset are\n",
    "uniformly distributed, the reduced dataset will have many dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A) Regular PCA is the default option, but it requires the data set to be loaded into memory; \n",
    "(b) Incremental PCA can handle large data sets, but its speed is slower than regular PCA, so if the data set can be put into \n",
    "    memory, Conventional PCA is more suitable. Incremental PCA can also be applied to online tasks: whenever a new data arrives, \n",
    "    PCA is quickly applied.\n",
    "(C) Random PCA can be used when you need to greatly reduce the dimensionality and the data set can be loaded into memory; it is \n",
    "    much faster than regular PCA.\n",
    "(D) Kernel-PCA is useful for non-linear data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How do you assess a dimensionality reduction algorithm's success on your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A dimensionality reduction algorithm is said to work well if it eliminates a significant number of dimensions from the dataset \n",
    "without losing too much information. Moreover, the use of dimensionality reduction in preprocessing before training the model \n",
    "allows measuring the performance of the second algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It can make sense to combine two Dimensionality reduction methods. One can use a fast projection methods to first get rid of \n",
    "useless dimensions ,and  then use a slow manifold learning methods (LLE) to ‘unfold’ then remaining dataset to even lower \n",
    "dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
