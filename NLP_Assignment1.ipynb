{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In one hot encoding, every word  which are part of the given text data are written in the form of vectors, constituting only of \n",
    "1 and 0 . So one hot vector is a vector whose elements are only 1 and 0. Each word is written or encoded as one hot vector, with\n",
    "each one hot vector being unique. This allows the word to be identified uniquely by its one hot vector and vice versa, that is \n",
    "no two words will have same one hot vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Bag-Of-Words representation is an algorithm that transforms the text into fixed-length vectors. This is possible by counting\n",
    "the number of times the word is present in a document. It represents the sentence as a bag of terms. It doesn’t take into \n",
    "account the order and the structure of the words, but it only checks if the words appear in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N-grams are continuous sequences of words or symbols or tokens in a document. In technical terms, they can be defined as the \n",
    "neighbouring sequences of items in a document. They come into play when we deal with text data in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF is a technique which is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag \n",
    "of Words technique which is good for text classification or for helping a machine read words in numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words in an input that doesn’t have a pre-trained vector. Such words are known as Out of Vocabulary Word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Embeddings are a method of extracting features out of text so that we can input those features into a machine learning \n",
    "model to work with text data. They try to preserve syntactical and semantic information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Explain Continuous bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are \n",
    "contextually accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Explain SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word.\n",
    "Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is \n",
    "input while context words are output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe stands for Global Vectors for word representation. It is an unsupervised learning algorithm developed by researchers at \n",
    "Stanford University aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus.   \n",
    "\n",
    "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics.  Unlike the \n",
    "occurrence matrix, the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the \n",
    "co-occurrence matrix represents a pair of words occurring together. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
