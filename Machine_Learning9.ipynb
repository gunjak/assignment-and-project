{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set.\n",
    "It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data \n",
    "transformations while also enhancing model accuracy.\n",
    "\n",
    "Feature engineering in ML consists of four main steps: Feature Creation, Transformations, Feature Extraction, and Feature \n",
    "Selection. \n",
    "Feature Creation: Creating features involves identifying the variables that will be most useful in the predictive model. This is\n",
    "    a subjective process that requires human intervention and creativity. Existing features are mixed via addition, subtraction,\n",
    "    multiplication, and ratio to create new derived features that have greater predictive power.  \n",
    "Transformations: Transformation involves manipulating the predictor variables to improve model performance; e.g. ensuring the \n",
    "    model is flexible in the variety of data it can ingest; ensuring variables are on the same scale, making the model easier\n",
    "    to understand; improving accuracy; and avoiding computational errors by ensuring all features are within an acceptable range\n",
    "    for the model. \n",
    "Feature Extraction: Feature extraction is the automatic creation of new variables by extracting them from raw data. The purpose\n",
    "    of this step is to automatically reduce the volume of data into a more manageable set for modeling. Some feature extraction\n",
    "    methods include cluster analysis, text analytics, edge detection algorithms, and principal components analysis.\n",
    "Feature Selection: Feature selection algorithms essentially analyze, judge, and rank various features to determine which \n",
    "    features are irrelevant and should be removed, which features are redundant and should be removed, and which features are \n",
    "    most useful for the model and should be prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection  essentially analyze, judge, and rank various features to determine which  features are irrelevant and should \n",
    "be removed, which features are redundant and should be removed, and which features are most useful for the model and should be \n",
    "prioritized.\n",
    "the various methods of function selection are:\n",
    "Feature Selection: Select a subset of input features from the dataset.\n",
    "Unsupervised: Do not use the target variable .\n",
    "Correlation\n",
    "Supervised: Use the target variable .\n",
    "Wrapper: Search for well-performing subsets of features.\n",
    "RFE\n",
    "Filter: Select subsets of features based on their relationship with the target.\n",
    "Statistical Methods\n",
    "Feature Importance Methods\n",
    "Intrinsic: Algorithms that perform automatic feature selection during training.\n",
    "Decision Trees\n",
    "Dimensionality Reduction: Project input data into a lower-dimensional feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods under filter:\n",
    "    Information gain.\n",
    "    Chi-square test:It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of \n",
    "        correlation or association between them using their frequency distribution.\n",
    "    Fisher’s Score.\n",
    "    Correlation coefficient.\n",
    "    Variance threshold\n",
    "    Mean absolute deviation.\n",
    "    dispersion coefficient\n",
    "Methods under wrapper:\n",
    "    Foreword selection: Forward selection is an iterative method in which we start with having no feature in the model. In each\n",
    "        iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve\n",
    "        the performance of the model.\n",
    "    Backward selection:In backward elimination, we start with all the features and removes the least significant feature at each\n",
    "        iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of\n",
    "        features\n",
    "    Bi-directional selection.\n",
    "    Exhaustive selection.\n",
    "    Recursive selection:It is a greedy optimization algorithm which aims to find the best performing feature subset. It \n",
    "        repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the \n",
    "        next model with the left features until all the features are exhausted. It then ranks the features based on the order of \n",
    "        their elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable \n",
    "to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the \n",
    "performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre-processing and normalizing text\n",
    "Following are some of the popular pre-processing techniques to pre-process, clean, and normalize the text.\n",
    "\n",
    "Text tokenization and lower casing\n",
    "Removing special characters\n",
    "Contraction expansion\n",
    "Removing stopwords\n",
    "Correcting spellings\n",
    "Stemming\n",
    "Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even if the two similar documents are far apart by the Euclidean distance because of the size they could still have a smaller \n",
    "angle between them. Smaller the angle, higher the similarity.\n",
    "The resemblance in cosine is 5.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the Hamming distance between two vectors is the number of bits we must change to change one into the other. The hamming gap is 2\n",
    "The smc is 0.75 and Jaccard index is 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. State what is meant by 'high-dimensional data set'? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High dimensional data refers to a dataset in which the number of features  is larger than the number of observations.\n",
    "Example:High dimensional data is common in healthcare datasets where the number of features for a given individual can be \n",
    "    massive.it’s common for the number of features to be larger than the number of observations.\n",
    "    \n",
    "It becomes impossible to find a model that can describe the relationship between the predictor variables and the response \n",
    "variable because we don’t have enough observations to train the model on.  \n",
    "\n",
    "To reduce problems:\n",
    "    Drop features with many missing values\n",
    "    Drop features with low variance\n",
    "    Drop features with low correlation with the response variable\n",
    "    Principal Components Analysis\n",
    "    Principal Components Regression\n",
    "    Ridge Regression\n",
    "    Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Make a few quick notes on:\n",
    "\n",
    "1.PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Principal component analysis,  is a statistical procedure that allows us to summarize the information content in large\n",
    "data tables by means of a smaller set of “summary indices” that can be more easily visualized and analyzed.\n",
    "2.In machine learning, feature vectors are used to represent numeric or symbolic characteristics, called features, of an object\n",
    "  in a mathematical, easily analyzable way. They are important for many different areas of machine learning and pattern \n",
    "  processing.\n",
    "3.This method is the combination of the filter method and wrapper methods which is fast as filter methods are and accurate as of\n",
    "the wrapper method. These methods are blended as part of the learning algorithm. They are most accurate because they overcome\n",
    "the drawbacks of filter and wrapper methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.\n",
    "sequential backward selection variant algorithm picks all the features from the input data and combines them in a set and \n",
    "sequentially removes them from the set until the removal of further features increases the criterion.\n",
    "In sequential forward selection  variant features are sequentially added to an empty set of features until the addition of extra\n",
    "features does not reduce the criterion.\n",
    "2.\n",
    "Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the \n",
    "usefulness of a subset of feature by actually training a model on it.\n",
    "3.\n",
    "the SMC counts both mutual presences  and mutual absence  as matches and compares it to the total number of attributes in the \n",
    "universe.\n",
    "The Jaccard  coefficient is a statistic used in understanding the similarities between sample sets. The measurement emphasizes\n",
    "similarity between finite sample sets, and is formally defined as the size of the intersection divided by the size of the union\n",
    "of the sample sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
