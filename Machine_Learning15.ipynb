{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised Learning is a category in which we feed labeled data as input to the machine learning model.\n",
    "\n",
    "Unsupervised learning is a category of machine learning in which we only have the input data to feed to the model but no \n",
    "corresponding output data.\n",
    "\n",
    "Semi-supervised learning is a category of machine learning in which we have input data, and only some of those input data are\n",
    "labeled as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spam filtering:The classification model could be a function that maps from an email text to a spam classification . Algorithms \n",
    "               such as Naive Bayes and Support Vector Machines can be used for classification.\n",
    "Sentiment analysis: A machine learning binary classification model can be trained to identify the sentiment  of a given text \n",
    "                document based on classification algorithms like Naïve Bayes, SVM etc.  \n",
    "Credit card fraud detection: A binary classification model can be used for credit card fraud detection where the historical \n",
    "                transactions data of a customer is analyzed using machine learning algorithms like Naïve Bayes, k-NN etc. \n",
    "Image classification:  To categorize photos into distinct categories, a multinomial classification model can be developed. \n",
    "Image sentiment analysis: Machine learning binary classification models based on machine learning algorithms can be created to \n",
    "                classify whether an image has a good or negative emotion/sentiment.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Support Vector Machine” is a supervised machine learning algorithm that can be used for both classification or regression \n",
    "challenges.In the SVM algorithm, we plot each data item as a point in n-dimensional space with the value of each feature being \n",
    "the value of a particular coordinate.Then, we perform classification by finding the hyper-plane that differentiates the two \n",
    "classes very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benefits:\n",
    "SVM works relatively well when there is a clear margin of separation between classes.\n",
    "SVM is more effective in high dimensional spaces.\n",
    "SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "SVM is relatively memory efficient\n",
    "\n",
    "Drawbacks:\n",
    "    \n",
    "SVM algorithm is not suitable for large data sets.\n",
    "SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "In cases where the number of features for each data point exceeds the number of training data samples, the SVM will \n",
    "underperform.\n",
    "As the support vector classifier works by putting data points, above and below the classifying hyperplane there is no \n",
    "probabilistic explanation for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "2.K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that\n",
    " is most similar to the available categories.\n",
    "3.K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data \n",
    " appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "4.K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
    "5.K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
    "6.It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the \n",
    "  dataset and at the time of classification, it performs an action on the dataset.\n",
    "7.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a \n",
    "  category that is much similar to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discuss the kNN algorithm's error rate and validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we choose k=1 we will pick up a lot of noise in the model. But if we increase value of k, you’ll notice that we achieve \n",
    "smooth separation or bias. This cleaner cut-off is achieved at the cost of miss-labeling some data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between\n",
    "the test data and all the training points. It then selects the k points which are closest to the test data. Once the points \n",
    "are selected, the algorithm calculates the probability  of the test point belonging to the classes of the k training points, and\n",
    "the class with the highest probability is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The k-nearest neighbor algorithm is imported from the scikit-learn package.\n",
    "Create feature and target variables.\n",
    "Split data into training and test data.\n",
    "Generate a k-NN model using neighbors value.\n",
    "Train or fit the data into the model.\n",
    "Predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision tree is a map of the possible outcomes of a series of related choices. It allows an individual or organization to \n",
    "weigh possible actions against one another based on their costs, probabilities, and benefits.\n",
    "There are three different types of nodes: chance nodes, decision nodes, and end nodes. A chance node, represented by a circle, \n",
    "shows the probabilities of certain results. A decision node, represented by a square, shows a decision to be made, and an \n",
    "end node shows the final outcome of a decision path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe the different ways to scan a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe in depth the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision tree algorithm falls under the category of supervised learning. They can be used to solve both regression and \n",
    "classification problems.\n",
    "Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and \n",
    "attributes are represented on the internal node of the tree.\n",
    "We can represent any boolean function on discrete attributes using the decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The inductive bias of a learning algorithm is the set of assumptions that the learner uses to  predict outputs of given inputs \n",
    "that it has not encountered.\n",
    "\n",
    "(1)max_depth: represents how deep your tree will be . More we increase the number, more will be the number of splits \n",
    "    and the possibility of overfitting.\n",
    "(2) min_samples_split: represents minimum number of samples required to split an internal node. If you use 100% of the samples \n",
    "    at each node, it will show underfitting, if you use 10%, it might show overfitting. Tune it accordingly.\n",
    "(3) min_samples_leaf: represents min. no. of samples required to be in the leaf node. The more we increase the number, more is \n",
    "    the possibility of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "    It can be used for both classification and regression problems.\n",
    "    It can capture nonlinear relationships.\n",
    "    They are very fast and efficient compared to KNN and other classification algorithms.\n",
    "    Less data preparation needed.In the decision tree, there is no effect by the outsider or missing data in the node of the\n",
    "    tree, that’s why the decision tree requires fewer data.\n",
    "    Normalization is not required in the Decision Tree.\n",
    "Disadvantages:\n",
    "    Take more time for training-time complexity to increase as the input increases.\n",
    "    Method of overfitting: If we discuss overfitting, it is one of the most difficult methods for decision tree models.\n",
    "    Concerning the decision tree split for numerical variables millions of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The target function has discrete output values.\n",
    "The training data may contain missing attribute values.\n",
    "The training data may contain errors.\n",
    "“Decision tree learning methods are robust to errors, both errors in classifications of the training examples and errors in the\n",
    "attribute values that describe these examples.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe in depth the random forest model. What distinguishes a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when\n",
    "building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate \n",
    "than that of any individual tree.\n",
    "Random forests differ from bagged trees by forcing the tree to use only a subset of its available predictors to split on in the\n",
    "growing phase. All the decision trees that make up a random forest are different because each tree is built on a different \n",
    "random subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOB Error is the number of wrongly classifying the OOB Sample.\n",
    "Out-of-bag error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted \n",
    "decision trees, and other machine learning models utilizing bootstrap aggregating . Bagging uses subsampling with replacement \n",
    "to create training samples for the model to learn from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
