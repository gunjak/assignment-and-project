{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A feature is an individual measurable property or characteristic of a phenomenon.\n",
    "In machine learning, features are individual independent variables that act like a input in your system. Actually, while making\n",
    "the predictions, models use such features to make the predictions. And using the feature engineering process, new features can \n",
    "also be obtained from old features in machine learning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature construction is a process which builds intermediate features from the original descriptors in a dataset. The aim is to \n",
    "build more efficient features for a machine data mining task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Describe how nominal variables are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The two most popular techniques are an Ordinal Encoding and a One-Hot Encoding are used for encoding nominal variable. \n",
    "In Label encoding, each label is converted into an integer value.\n",
    ". In one hot encoding, for each level of a categorical feature, we create a new variable. Each category is mapped with a binary\n",
    "variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discretization: It is the process of transforming continuous variables into categorical variables by creating a set of intervals\n",
    ", which are contiguous, that span over the range of the variable’s values. It is also known as “Binning”, where the bin is an \n",
    "analogous name for an interval.\n",
    "\n",
    "Benefits of Discretization:\n",
    "\n",
    "Equal width binning: It is also known as “Uniform Binning” since the width of all the intervals is the same. The algorithm \n",
    "    divides the data into N intervals of equal size. \n",
    "    \n",
    "Equal frequency binning: It is also known as “Quantile Binning”. The algorithm divides the data into N groups where each group \n",
    "contains approximately the same number of values.\n",
    "\n",
    "Consider, we want 10 bins, that is each interval contains 10% of the total observations.\n",
    "Here the width of the interval need not necessarily be equal.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on\n",
    "a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the \n",
    "evaluation criterion.\n",
    "Advantages:\n",
    "    Less prone to local optima\n",
    "    Interacts with classifier\n",
    "    model feature dependencies\n",
    "    higher performace  accuracy than filter\n",
    "    \n",
    "Disadvantages:\n",
    "    computationally intensive\n",
    "    Discriminative power\n",
    "    lower shorter training time\n",
    "    higher risk of overfitting than deterministic order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. When is a feature considered irrelevant? What can be said to quantify it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When a feature has large number of null value.\n",
    "when a feature has not  normal distribution.\n",
    "when a feature is not correlated with target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. When is a function considered redundant? What criteria are used to identify features that could be redundant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An attribute is called redundant if it can be derived from any other attribute or set of attributes. Inconsistencies in \n",
    "attribute or dimension naming can also lead to the redundancies in data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What are the various distance measurements used to determine feature similarity?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine Similarity:\n",
    "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size\n",
    "2) Manhattan distance:\n",
    "Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their \n",
    "Cartesian coordinates\n",
    "3) Euclidean distance:\n",
    "The Euclidean distance between two points in either the plane or 3-dimensional space measures the length of a segment connecting\n",
    "the two points\n",
    "4) Minkowski distance\n",
    "Minkowski distance is a generalisation of the Euclidean and Manhattan distances.\n",
    "5) Jaccard similarity:\n",
    "The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient is a statistic used for gauging \n",
    "the similarity and diversity of sample sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distance: Euclidean distance is calculated as the square root of the sum of the squared differences between a new \n",
    "point (x) and an existing point (y).\n",
    "\n",
    "The Euclidean distance or Euclidean metric is the \"ordinary\" (i.e.straight-line) distance between two points in Euclidean space.\n",
    "The Euclidean distance between points p and q is the length of the line segment connecting them. \n",
    "\n",
    "Manhattan Distance: This is the distance between real vectors using the sum of their absolute difference.\n",
    "\n",
    "It is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Distinguish between feature transformation and feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is a process that chooses a subset of features from the original features so that the feature space is \n",
    "optimally reduced according to a certain criterion. Feature extraction/construction is a process through which a set of new \n",
    "features is created. They are used either in isolation or in combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a hybrid approach for feature selection called based on genetic algorithms that employs a target learning algorithm to evaluate\n",
    "features, a wrapper method. The advantages of this approach include the ability to accommodate multiple feature selection \n",
    "criteria and find small subsets of features that perform well for the target algorithm.\n",
    "\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a \n",
    "binary classifier system as its discrimination threshold is varied.The ROC curve is created by plotting the true positive rate \n",
    "against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall\n",
    "or probability of detection. The false-positive rate is also known as probability of false alarm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
